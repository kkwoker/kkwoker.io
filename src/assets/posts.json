[
  {
    "routeKey": "loremipsum",
    "title": "Lorem Ipsum",
    "date": "2018-01-08T05:01:47.000Z",
    "tags": [
      "mathematics",
      "algorithms"
    ],
    "draft": true,
    "image": {
      "feature": "/images/abstract-10.jpg"
    },
    "html": "<p>A perfect matching S is a matching with the property that each member of M and\neach member of W appears in exactly one pair in S.</p>\n<p>blahblahla</p>\n<h2 id=\"gale-shapely-algorithm\">Gale-Shapely Algorithm</h2>\n<pre><code>Initially all m ∈ M and w ∈ W are free\nWhile there is a man m who is free and hasn&#39;t proposed to every woman\n  Choose such a man m\n  Let w be the highest-ranked woman in m&#39;s preference list to whom m has not yet\n  proposed\n  If w is free then\n    (m, w) become engaged\n  Else w is currently engaged to m&#39;\n    If w prefers m&#39; to m then\n      m remains free\n    Else w prefers m to m&#39;\n      (m, w) become engaged\n      m&#39; becomes free\n    EndIf\n  EndIf\nEndWhile\nReturn the set S of engaged pairs\n</code></pre><h3 id=\"thm-0-the-algorithm-always-stops-\">Thm 0. The algorithm always stops.</h3>\n<p>Finitely number of preferences</p>\n<h3 id=\"thm-1-the-algorithm-arranges-stable-marriages-\">Thm 1. The algorithm arranges stable marriages.</h3>\n<p>Proof: Show that no woman can trade up, the husband is the best she can do.</p>\n<p>Each man a given woman prefers rejected her on the way. QED</p>\n<h3 id=\"thm-2-the-algorithm-is-man-optimal\">Thm 2. The algorithm is man-optimal</h3>\n<h3 id=\"collorary-the-algorithm-is-woman-pessimal\">Collorary: The algorithm is woman-pessimal</h3>\n"
  },
  {
    "routeKey": "first-post",
    "title": "IT IS ALIVE!",
    "imageSrc": "https://emojipedia-us.s3.dualstack.us-west-1.amazonaws.com/socialmedia/apple/155/party-popper_1f389.png",
    "date": "2018-04-09T00:00:00.000Z",
    "draft": false,
    "html": "<p>After months of flip flopping between static site themes, I&#39;ve finally\nfound my solution: Build my own website! This gives me complete rein\nover all pixels, structure, and themes. Building my own site also gives\nme a place to deploy random application ideas. Something I&#39;m hoping to\nget built is some sort of Machine Learning demonstration with\nrecommending items based on a group of selected items. Stay tuned to see\nif I commit to this!</p>\n<p>Hurray!</p>\n"
  },
  {
    "routeKey": "algorithms-cheat-sheet",
    "title": "Algorithms Cheat Sheet",
    "imageSrc": "https://upload.wikimedia.org/wikipedia/en/4/41/Clrs3.jpeg",
    "date": "2018-04-11T00:00:00.000Z",
    "tags": [
      "mathematics",
      "algorithms"
    ],
    "draft": false,
    "html": "<p>This is a cheatsheet for my CMPT307 - Algorithms and Data Structures\nclass. In this class, we learn about Greedy Algorithms, Dynamic\nProgramming, Graph Algorithms and NP completeness.</p>\n<p><a href=\"https://www.math.uh.edu/~ilya/class/useful_summations.pdf\">Useful Summation Series</a></p>\n<p><img src=\"http://latex.codecogs.com/gif.latex?%5Csum_%7Bk%3D0%7D%5E%7Bn%7Da%5Ek%20%3D%20%5Cfrac%7Ba%5E%7Bn&plus;1%7D%20-%201%7D%7Ba-1%7D\" alt=\"finite sum a k\"></p>\n<h2 id=\"data-structures\">Data Structures</h2>\n<h4 id=\"binary-tree\">Binary Tree</h4>\n<h4 id=\"max-min-heaps-and-priority-queues\">Max/min Heaps and Priority Queues</h4>\n<h4 id=\"max-heapify\">Max-heapify</h4>\n<pre><code>BUILD-MAX-HEAP(A):\n    for i = n/2 downto 1\n        MAX_HEAPIFY(A, i)\n</code></pre><pre><code>MAX_HEAPIFY(A, i):\n    if A[i] &gt; A[left(i)] OR A[i] &gt; A[right(i)] then Heapify-down(A, i)\n</code></pre><p>Observe MAX_HEAPIFY takes O(1) for nodes that are 1 level above the leaves.</p>\n<p>In general, O(l) time for nodes that are l levels above the leaves.</p>\n<p>n/4 nodes with level 1,</p>\n<p>n/8 nodes with level 2,</p>\n<p>...</p>\n<p>1 node at log(n) level</p>\n<p>Total amount of work in the for loop:\nn/4 <em> (1c) + n/8 </em> (2c) + n/16 * 3c + 1(clogn)\nset n/4 = 2^k\nextract a arithmetic series constant out, we get O(n)</p>\n<h4 id=\"linked-lists\">Linked Lists</h4>\n<h2 id=\"growth-of-functions\">Growth of Functions</h2>\n<h4 id=\"asymptotic-notation\">Asymptotic notation</h4>\n<p>ϴ-notation asymptotic tight bound</p>\n<p>O-notation asymptotic upper bound</p>\n<p>Ω-notation asymptotic lower bound</p>\n<p><img src=\"http://latex.codecogs.com/gif.latex?O%28%5Clog%7Bn%7D%29%20%5Cleq%20O%28n%29%20%5Cleq%20O%28n%5Clog%7Bn%7D%29%20%5Cleq%20O%28n%5Ek%29%20%5Cleq%20O%28k%5En%29%20%5Cleq%20O%28n%21%29\" alt=\"O(\\log{n}) \\leq O(n) \\leq  O(n\\log{n}) \\leq  O(n^k) \\leq O(k^n) \\leq  O(n!)\"></p>\n<p><img src=\"http://latex.codecogs.com/gif.latex?%5Cbinom%7Bn%7D%7Bk%7D%20%3D%20O%28n%5Ek%20/%20k%21%29%20%3D%20O%28n%5Ek%29\" alt=\"\\binom{n}{k} = O(n^k / k!) = O(n^k)\"></p>\n<h2 id=\"elementary-graph-algorithms\">Elementary Graph Algorithms</h2>\n<h4 id=\"breadth-first-search\">Breadth First Search</h4>\n<pre><code>BFS(G, s)\n    for each v ∃ V - {s}\n        v.color = WHITE\n        v.d = INFINITY\n        v.pi = NIL\n    s.color = GRAY\n    s.d = 0\n    s.pi = NIL\n    Q = 0\n    ENQUEUE(Q, s)\n    while Q != 0\n        u = DEQUEUE(Q)\n        for each v ∃ Adjacent(u)\n            if v.color == WHITE\n                v.color = GRAY\n                v.d = u.d + 1\n                v.pi = u\n                ENQUEUE(Q, v)\n        u.color = BLACK\nO(|V| + |E|)\n</code></pre><h4 id=\"depth-first-search\">Depth First Search</h4>\n<pre><code>DFS(G)\n    for each u ∃ V\n        u.color = WHITE\n        u.pi = NIL\n    time = 0\n    for each vertex u ∃ V\n        if u.color == WHITE\n            DFS-VISIT(G, u)\nDFS-VISIT(G, u)\n    time = time + 1\n    u.d = time\n    u.color = GRAY\n    for each v ∃ Adjacent(u)\n        if v.color == WHITE\n            v.pi = u\n            DFS-VISIT(G, v)\n    u.color = BLACK\n    time = time + 1\n    u.f = time\nO(|V| + |E|)\n\n</code></pre><h4 id=\"topological-sorting\">Topological Sorting</h4>\n<pre><code>  call DGS(G) to compute finish times v.f for each vertex v\n  as each vertex is finished, insert it onto the front of a linked list\n  return the linked list of vertices\n</code></pre><h2 id=\"divide-and-conquer\">Divide and Conquer</h2>\n<h4 id=\"substitution-method\">Substitution Method</h4>\n<p><img src=\"http://latex.codecogs.com/svg.latex?%5Ctext%7BLet%20%7D%20m%20%3D%20%5Clog%7Bn%7D%20%5C%5C%20%5Ctext%7BLet%20%7D%20S%28m%29%20%3D%20%5Cfrac%7BT%282%5Em%29%7D%7B2%5Em%7D%5C%5C%20%5Cbegin%7Balign*%7D%20T%28n%29%20%26%3D%20%5Csqrt%7Bn%7DT%28%5Csqrt%7Bn%7D%29%20&plus;%20n%20%5C%5C%20T%282%5Em%29%20%26%3D%202%5E%7Bm/2%7DT%282%5E%7Bm/2%7D%29%20&plus;%202%5Em%20%5C%5C%20%5Cfrac%7BT%282%5Em%29%7D%7B2%5Em%7D%20%26%3D%20%5Cfrac%7BT%282%5E%7Bm/2%7D%29%7D%7B2%5E%7Bm/2%7D%7D%20&plus;%201%5C%5C%20S%28m%29%20%26%3D%20S%28%5Cfrac%7Bm%7D%7B2%7D%29%20&plus;%201%5C%5C%20S%28m%29%20%26%3D%20%5CTheta%20%28%5Clog%7Bm%7D%29%20%5C%5C%20%5Cfrac%7BT%282%5Em%29%7D%7B2%5Em%7D%20%26%3D%20%5CTheta%20%28%5Clog%7Bm%7D%29%20%5C%5C%20T%28n%29%20%26%3D%20n%5CTheta%20%28%5Clog%7B%5Clog%7Bn%7D%7D%29%20%5C%5C%20T%28n%29%20%26%3D%20%5CTheta%20%28n%5Clog%7B%5Clog%7Bn%7D%7D%29%20%5C%5C%20%5Cend%7Balign*%7D\" alt=\"\\text{Let } m = \\log{n} \\\\\n\\text{Let } S(m) = \\frac{T(2^m)}{2^m}\\\\\n\\begin{align*}\nT(n)  &amp;= \\sqrt{n}T(\\sqrt{n}) + n \\\\\nT(2^m) &amp;= 2^{m/2}T(2^{m/2}) + 2^m \\\\\n\\frac{T(2^m)}{2^m} &amp;= \\frac{T(2^{m/2})}{2^{m/2}} + 1\\\\\nS(m) &amp;= S(\\frac{m}{2}) + 1\\\\\nS(m) &amp;= \\Theta (\\log{m}) \\\\\n\\frac{T(2^m)}{2^m} &amp;= \\Theta (\\log{m}) \\\\\nT(n) &amp;= n\\Theta (\\log{\\log{n}}) \\\\\nT(n) &amp;= \\Theta (n\\log{\\log{n}}) \\\\\n\\end{align*}\"></p>\n<h4 id=\"recursion-tree-method\">Recursion-Tree Method</h4>\n<h4 id=\"master-theorem\">Master Theorem</h4>\n<p><img src=\"http://latex.codecogs.com/gif.latex?T%28n%29%20%3D%20aT%28n/b%29%20&plus;%20f%28n%29\" alt=\"master theorem\"></p>\n<ol>\n<li><p>If <img src=\"http://latex.codecogs.com/gif.latex?f%28n%29%20%3D%20O%28n%5E%7Blog_b%7Ba-%5Cepsilon%7D%7D%29\" alt=\"\"> then <img src=\"http://latex.codecogs.com/gif.latex?T%28n%29%20%3D%20%5CTheta%28n%5E%7Blog_b%7Ba%7D%7D%29\" alt=\"\"></p>\n</li>\n<li><p>If <img src=\"http://latex.codecogs.com/gif.latex?f%28n%29%20%3D%20%5CTheta%28n%5E%7Blog_b%7Ba%7D%7D%29\" alt=\"\"> then <img src=\"http://latex.codecogs.com/gif.latex?T%28n%29%20%3D%20%5CTheta%28n%5E%7Blog_b%7Ba%7D%7D%5Clg%20%7Bn%7D%29\" alt=\"\"></p>\n</li>\n<li><p>If <img src=\"http://latex.codecogs.com/gif.latex?f%28n%29%20%3D%20%5COmega%20%28n%5E%7Blog_b%7Ba&plus;%5Cepsilon%20%7D%7D%29\" alt=\"\"> then <img src=\"http://latex.codecogs.com/gif.latex?T%28n%29%20%3D%20%5CTheta%28f%28n%29%29\" alt=\"\"></p>\n</li>\n</ol>\n<h4 id=\"muster-theorem\">Muster Theorem</h4>\n<p><img src=\"http://latex.codecogs.com/gif.latex?T%28n%29%20%3D%20aT%28n%20-%20b%29%20&plus;%20f%28n%29\" alt=\"muster theorem\"></p>\n<ol>\n<li>If <img src=\"http://latex.codecogs.com/gif.latex?a%3C1\" alt=\"\"> then <img src=\"http://latex.codecogs.com/gif.latex?T%28n%29%20%3D%20O%28n%5Ed%29\" alt=\"\"></li>\n<li>If <img src=\"http://latex.codecogs.com/gif.latex?a%3D1\" alt=\"\"> then <img src=\"http://latex.codecogs.com/gif.latex?T%28n%29%20%3D%20O%28n%5E%7Bd%20&plus;%201%7D%29\" alt=\"\"></li>\n<li>If <img src=\"http://latex.codecogs.com/gif.latex?a%3E1\" alt=\"\"> then <img src=\"http://latex.codecogs.com/gif.latex?T%28n%29%20%3D%20O%28n%5E%7Bd%7Da%5E%7Bn/b%7D%29\" alt=\"\"></li>\n</ol>\n<h2 id=\"greedy-algorithms\">Greedy Algorithms</h2>\n<h4 id=\"minimum-spanning-trees\">Minimum Spanning trees</h4>\n<pre><code>GENERIC-MST(G, w)\n    A = 0\n    while A does not form a spanning tree\n        find an edge (u, v) that is safe for A\n        A = A U {(u, v)}\n    return A\n</code></pre><pre><code>MST-KRUSKAL(G, w)\n    A = 0\n    for each v in V\n        MAKE-SET(v)\n    sort the edges of E into increasing order by weight w\n    for each edge (u, v) in E, taken in increasing order by weight\n        if FIND-SET(u) != FIND-SET(v)\n            A = A U {(u, v)}\n            UNION(u, v)\n    return A\n</code></pre><pre><code>MST-PRIM(G, w, r)\n    for each u in V\n        u.key = INFINITY\n        u.pi = NIL\n    r.key = 0\n    Q = V\n    while Q != 0\n        u = EXTRACT-MIN(Q)\n        for each v in Adjacent(u)\n            if v is in Q and w(u, v) &lt; v.key\n                v.pi = u\n                v.key = w(u, v)\n</code></pre><h4 id=\"activity-scheduling\">Activity Scheduling</h4>\n<pre><code>GREEDY-ACTIVITY-SELECTOR(s, f)\n    n = s.length\n    A = {a_1}\n    k = 1\n    for m = 2..n\n        if s[m] &gt;= f[k]\n            A = A U {a_m}\n            k = m\n    return A\n\n</code></pre><h4 id=\"cut-and-paste-argument\">Cut and Paste argument</h4>\n<p>eg. Consider any nonempty subproblem S_k, and let a_m be an activity in S_k with\nthe earliest finish time. Then a_m is included in some maximum-size subset of mutually compatible activities of S_k</p>\n<p>PF: Let A_k be a maximum-size subset of mutually compatible activities in S_k and let a_j be the activity in A_k with the earliest finish time. If a_j = a_m, we are done since we have shown that a_m is in some maximum-size subset of mutually compatible activities of S_k. If a_j != a_m let the set A&#39;_k = A_k - {a_j} U {a_m} be A_k be substituting a_m for a_j. The activities in A&#39;_k are disjoint, which follows because the activities in A_k are disjoint, a_k is the first activity in A_k to finish, and f_m &lt;= f_j. Since |A&#39;_k| = |A_k|, we conclude that A&#39;_k is a maximum size subset of mutually compatible activies of S_k and it includes a_m.</p>\n<h2 id=\"single-source-shortest-paths\">Single Source Shortest Paths</h2>\n<pre><code>INITIALIZE-SINGLE-SOURCE(G, s)\n    for each v in V\n        v.d = INFINITY\n        v.pi = NIL\n    s.d = 0\n</code></pre><pre><code>RELAX(u, v, w)\n    if v.d &gt; u.d + w(u, v)\n        v.d = u.d + w(u, v)\n        v.pi = u\n\n</code></pre><h4 id=\"dijkstra-s\">Dijkstra&#39;s</h4>\n<pre><code>G is a directed or undirected graph (V, E) with no negative cycles reachable from s\nDIJKSTRA(G, w, s)\n    INITIALIZE-SINGLE-SOURCE(G, s)\n    S = 0\n    Q = V\n    while Q != 0\n        u = EXTRACT-MIN(Q)\n        S = S U {u}\n        for each v in Adjacent(u)\n            RELAX(u, v, w)\n</code></pre><h4 id=\"bellman-ford\">Bellman-Ford</h4>\n<pre><code>G is a directed graph\nBELLMAN-FORD(G, w, s)\n    INITIALIZE-SINGLE-SOURCE(G, s)\n        for i = 1 to |V| - 1\n            for each edge (u, v) in E\n                RELAX(u, v, w)\n        for each edge (u, v) in E\n            if v.d &gt; u.d + w(u, v)\n                return FALSE &quot;There exists a negative weight cycle reachable by s&quot;\n        return TRUE &quot;There is no negative weight cycles reachable by s&quot;\n\n</code></pre><h2 id=\"dynamic-programming\">Dynamic Programming</h2>\n<h4 id=\"elements-of-dynamic-programming\">Elements of dynamic programming</h4>\n<h4 id=\"longest-common-subsequence\">Longest common subsequence</h4>\n<h2 id=\"maximum-flow\">Maximum Flow</h2>\n<h4 id=\"max-flow-min-cut-theorem\">Max-Flow Min-Cut Theorem</h4>\n<p>If f is a flow in a flow network G = (V, E) with source s and sink t, then the following are euivalent:</p>\n<ol>\n<li>f is a maximum flow in G.</li>\n<li>The residual network Gf contains no augmenting paths.</li>\n<li>|f| = c(S, T) for some cut (S, T) of G.</li>\n</ol>\n<h4 id=\"ford-fulkerson-method\">Ford-Fulkerson method</h4>\n<pre><code>(u, v).f is the flow along edge (u, v)\nFORD-FULKERSON(G, s, t)\n    for each edge (u, v) in E\n        (u, v).f = 0\n    while there exists a path p from s to t in the residual network Gf\n        cf(p) = min{ cf(u, v): (u, v) is in p }  // Take the bottleneck edge in p\n        for each edge (u, v) in p\n            if (u, v) in E\n                (u, v).f = (u, v).f + cf(p)\n            else\n                (v, u).f = (v, u).f - cf(p)\n\n</code></pre><p>O(|E|C) where C is the maximum flow, since the flow value increases by at least 1 unit each iteration</p>\n<ul>\n<li>Capacity Scaling ~&gt; O(E^2logC) (Looks for the edges with the highest flows first)</li>\n<li>Edmonds-Karp ~&gt; O(VE^2) (uses BFS to find the augmenting path)</li>\n</ul>\n<h4 id=\"maximum-bipartite-matching\">Maximum Bipartite matching</h4>\n<h2 id=\"np-completeness\">NP-Completeness</h2>\n<h4 id=\"sat\">SAT</h4>\n<h4 id=\"independent-set\">Independent Set</h4>\n<h4 id=\"3-colour\">3-colour</h4>\n<h4 id=\"proof-techniques\">Proof techniques</h4>\n<h2 id=\"approximation-algorithms\">Approximation Algorithms</h2>\n<h4 id=\"vertex-cover\">Vertex Cover</h4>\n<pre><code>C = 0\nE&#39; = G.E\nwhile E&#39; != 0\n    let (u, v) be an arbitrary edge of E&#39;\n    C = C ∪ {u, v}\n    remove from E&#39; every edge incident on either u or v\nreturn C\n</code></pre><p>vertex cover whose size is guaranteed to be no more than twice the size of an optimal vertex cover</p>\n<p>Since once an edge is chosen, all other edges incident on its endpoints are deleted from E&#39;, no two edges in C are covered by the same vertex from C_opt</p>\n<h2 id=\"randomized-algorithms\">Randomized Algorithms</h2>\n"
  },
  {
    "html": ""
  },
  {
    "routeKey": "haskell-as-a-functional-programming-language",
    "title": "Haskell as a Functional Programming Language",
    "imageSrc": "https://www.fpcomplete.com/hubfs/haskell_logo.svg?t=1539735814409",
    "date": "2018-06-05T00:00:00.000Z",
    "tags": [
      "programming"
    ],
    "draft": false,
    "html": "<p>Haskell is a purely functional programming language. You can&#39;t say\nyou&#39;ve learned functional programming until you have learned a purely\nfunctional language. There are no concepts of for loops or while loops.\nYou must use recursion! There are no variables! Only arguments to\nfunctions!</p>\n<p>Here&#39;s some of the exercises I completed in Haskell.</p>\n<p>My final Haskell assignment was to build a rainbow table password\ndecoder. Check it out\n<a href=\"https://github.com/kkwoker/cs-summer2018/tree/master/CMPT383/assign1\">here</a>.</p>\n<p>For a comprehensive introduction to Haskell, I recommend the e-book\n<a href=\"http://learnyouahaskell.com/chapters\">Learn you Haskell for Great\nGood!</a>.</p>\n"
  },
  {
    "routeKey": "brief-notes-on-lda-nmf",
    "title": "Brief Notes: Topic Modelling with LDA and NMF",
    "imageSrc": "https://fr.mathworks.com/help/examples/textanalytics/win64/FitLDAModelExample_01.png",
    "date": "2019-01-25T00:00:00.000Z",
    "tags": [
      "data science",
      "machine learning"
    ],
    "draft": false,
    "html": "<p>Topic modelling is a method to extract grouping information from a set of text\ndocuments. The topics that are generated are representative of the set\nof documents. Two approaches are typically used: Latent Dirichlet\nAllocation (LDA) and Non-negative Matrix Factorization. This article\nwill introduce and examine these two topics.</p>\n<h2 id=\"topic-modelling\"><a href=\"#topic-modelling\">Topic Modelling</a></h2>\n<ul>\n<li>Find topics that are representative of a set of documents.</li>\n</ul>\n<h2 id=\"latent-dirichlet-allocation\"><a href=\"#latent-dirichlet-allocation\">Latent Dirichlet Allocation</a></h2>\n<p>Topics are represented by latent topics.\nTopics are represented by a distribution over words.</p>\n<ul>\n<li>Generative probabilistic model for collections of discrete data.</li>\n<li><p>Three-level hierarchical <em>Bayesian model</em></p>\n<ul>\n<li>each item is modelled as a finite mixture over an underlying set of\ntopic probabilities.</li>\n<li>topic probabilities represent a document.</li>\n</ul>\n</li>\n<li><p>is a mixture model. words and documents are &quot;exchangeable&quot;. The order\nof words in document, or ordering of documents in a corpus is not\nimportant.</p>\n</li>\n</ul>\n<p>LDA assumes that each document is generated by the following generative\nprocess:</p>\n<pre><code>  To generate a document:\n    1. Randomly choose a distribution over topics\n       (a distribution over a distribution).\n    2. For each word in a document:\n        a. Randomly choose a topic from the distribution over topics\n        b. Randomly choose a word from the corresponding topic\n           (distribution over the vocabulary).\n\n</code></pre><p>words are generated independently of other words, hence, a unigram\nbag-of-words model.</p>\n<p>The generative process finds the <em>joint distribution</em> of the hidden and\nobserved variables.</p>\n<p><em>Bayesian Model</em></p>\n<p><img\nsrc=\"https://wiki.ubc.ca/images/8/85/LDA_Graphical_Representation_in_Plate_Notation.png\"\n/></p>\n<p><img\nsrc=\"https://wiki.ubc.ca/api/rest_v1/media/math/render/svg/7ab3c8505c8a32500e5eea3e458824c102f9d7bc\"\n/></p>\n<p>Gibbs Sampling is a method to learn these</p>\n<h2 id=\"non-negative-matrix-factorization\"><a href=\"#non-negative-matrix-factorization\">Non-negative Matrix Factorization</a></h2>\n<p>Document-term matrix is approximately factorized into term-feature and\nfeature-document matrices.</p>\n<p>Reference</p>\n<p><a href=\"https://wiki.ubc.ca/Course:CPSC522/A_Comparison_of_LDA_and_NMF_for_Topic_Modeling_on_Literary_Themes\">https://wiki.ubc.ca/Course:CPSC522/A_Comparison_of_LDA_and_NMF_for_Topic_Modeling_on_Literary_Themes</a></p>\n"
  },
  {
    "routeKey": "baxter-the-kitty",
    "title": "Introducing Baxter the Kitty!",
    "imageSrc": "/images/baxter.JPG",
    "imageAlt": "Hi Baxter!",
    "date": "2019-01-30T00:00:00.000Z",
    "tags": [
      "life"
    ],
    "draft": false,
    "html": "<p>This little kitty is 6 months old when I got him.\nHis full name is Alabaster, but I named him Baxter for short.\nBaxter loves cuddling and playing with his rainbow sparkler stick.\nIn his free time, Baxter likes to take naps, jump on tables, and plays\nwith his scratching post.</p>\n"
  },
  {
    "html": "<p>January 29th</p>\n<p>World champ Backgammon\nDigit Recognition 99.26% accuracy</p>\n<p>neurons behave like logic gates, but with continuous values</p>\n<p>linear weighted input in the neural net</p>\n<p>in_j is input.</p>\n<p>a_j is the output, but it is called activation</p>\n<p>activation function is some nonliear function. like sigmoid</p>\n<p>Sigmoid, now is not so good.\nRadial basis function\nGaussian function --&gt; at any point we measure the distance from two\npoints.</p>\n<p>Rectified Linear Unit --&gt; useful for deep learning.</p>\n<p>For XOR, even if we have multiple hidden units, since we have linear activation\nfunctions, we cannot solve the XOR problem.</p>\n<p>Now given the sigmoid activation functions, it is able to solve it.</p>\n<p>Neural net is performing a function composition\ntwo opposite facing sigmoids is a ridge.</p>\n<p>two ridges make a bump.</p>\n<p>The first two neurons make the ridges, the next layer makes the bump.</p>\n<p>Two hidden units. Visualize the dark is strong weights, light is less\nweight.</p>\n<p>common trick is to add noise to input to force neural net to learn to be\nrobust. Learn the features that are important, not the ones that are in\nthe background.</p>\n<p>Image analysis tasks.</p>\n<p>CNN\nfeature visualization of vonc net trained on ImageNet.</p>\n<p>low level feature -&gt; mid level feature -&gt; high level feature</p>\n<p>even the high level features are somewhat hard to interpret.\nend is a trainable classifier. give me a label.</p>\n<p>Measuring training error.</p>\n<p>parametrer optimizationg</p>\n<p>error function is super non convex.</p>\n<p>no closed form --&gt; use gradient descent.</p>\n<p>What is the derivative of my last output wrt to a weight in the begin.</p>\n<p>Credit assignment problem in AI. Which weight is responsible. How much\nis each weight responsible to the output?</p>\n<p>Back propagation.\nlook at weights connected to output nodes.</p>\n<p>How wrong is the output of the hidden units?\nChange the weight that feeds into this hidden unit.</p>\n<p>Output node, you can see how wrong you are. But how do you see how wrong\nhidden nodes are?\n--&gt; Back propagation....</p>\n<p>For output node k with activation a_k = g(in_k)</p>\n<p>g&#39;(in_k)(t_k - a_k)</p>\n<p>g&#39; is the derivative of error wrt in_i</p>\n<p>Theory: backpropagation <em>implements</em> gradient descent.</p>\n<p>correctness proof for backprop.</p>\n<p>in_j = a_i * w_ij\nd(in_j)/d(w_ij) = a_i</p>\n<p>d(in_j)/d(a_i) = w_ij</p>\n<p>d(in_j)/d(in_i) = deriv wrt in_i of g(in_i) <em> w_ij = d(in_j)/d(a_i) </em> d(a_i)/d(in_i) = w_ij * g&#39;(in_i)</p>\n<p>Show that\n-dE_n/w_ij = del[j] * a_i</p>\n<p>del[j] is the error of the output node j</p>\n<p>theorem\nfor each node j, we have del[j] = -dE_n/in_j\nshow this equality.</p>\n<p>-d(E_n)/d(w_ij) = -d(E_n)/d(in_j) <em> d(in_j)/d(w_ij) = del[j] </em> a_i</p>\n<p>if we can prove this, then we can show that back propagation is equal to\ngradient descent.</p>\n<p>backward induction. claim is true for output nodes. --&gt; show this?</p>\n<p>del[k] === g&#39;(in_k)(t_k - a_k)</p>\n<p>del[k] = 1/2(t_k - a_k)^2\nd</p>\n"
  }
]