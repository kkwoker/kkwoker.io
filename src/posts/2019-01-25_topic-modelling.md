---
routeKey: "brief-notes-on-lda-nmf"
title: "Brief Notes: Topic Modelling with LDA and NMF"
imageSrc: "https://fr.mathworks.com/help/examples/textanalytics/win64/FitLDAModelExample_01.png"
date: 2019-01-25
tags: [data science, machine learning]
draft: true
---

Topic modelling is a method to extract grouping information from a set of text
documents. The topics that are generated are representative of the set
of documents. Two approaches are typically used: Latent Dirichlet
Allocation (LDA) and Non-negative Matrix Factorization. This article
will introduce and examine these two topics.


[Topic Modelling](#topic-modelling)
-----------------------------------

- Find topics that are representative of a set of documents.

[Latent Dirichlet Allocation](#latent-dirichlet-allocation)
-----------------------------------------------------------

Topics are represented by latent topics.
Topics are represented by a distribution over words.

- Generative probabilistic model for collections of discrete data.
- Three-level hierarchical _Bayesian model_
  - each item is modelled as a finite mixture over an underlying set of
  topic probabilities.
  - topic probabilities represent a document.

- is a mixture model. words and documents are "exchangeable". The order
of words in document, or ordering of documents in a corpus is not
important.

LDA assumes that each document is generated by the following generative
process:

```
  To generate a document:
    1. Randomly choose a distribution over topics
       (a distribution over a distribution).
    2. For each word in a document:
        a. Randomly choose a topic from the distribution over topics
        b. Randomly choose a word from the corresponding topic
           (distribution over the vocabulary).

```

words are generated independently of other words, hence, a unigram
bag-of-words model.


The generative process finds the _joint distribution_ of the hidden and
observed variables.

_Bayesian Model_

<img
src="https://wiki.ubc.ca/images/8/85/LDA_Graphical_Representation_in_Plate_Notation.png"
/>

<img
src="https://wiki.ubc.ca/api/rest_v1/media/math/render/svg/7ab3c8505c8a32500e5eea3e458824c102f9d7bc"
/>

Gibbs Sampling is a method to learn these

[Non-negative Matrix Factorization](#non-negative-matrix-factorization)
-----------------------------------------------------------------------

Document-term matrix is approximately factorized into term-feature and
feature-document matrices.

Reference

https://wiki.ubc.ca/Course:CPSC522/A_Comparison_of_LDA_and_NMF_for_Topic_Modeling_on_Literary_Themes
